{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning"
      ],
      "metadata": {
        "id": "KLQMt4OYPqzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Assignment"
      ],
      "metadata": {
        "id": "dI9OqP6lPtjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:  What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.**"
      ],
      "metadata": {
        "id": "AOGfviXVPy-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** **Ensemble Learning**  in machine learning is a strategic delivery model where multiple base learners (models) are orchestrated to work in tandem, to outperform any single model operating in isolation.\n",
        "\n",
        "**(Key Idea)**:\n",
        "\n",
        "The central thesis is collective intelligence: by aggregating diverse models, the ensemble mitigates individual weaknesses and amplifies overall performance. In enterprise terms, it’s a risk-diversification and performance-optimization strategy."
      ],
      "metadata": {
        "id": "hoLjV7epWehR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Bagging and Boosting?**"
      ],
      "metadata": {
        "id": "ByJmu3zQP4fM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** **Bagging (Bootstrap Aggregating)**\n",
        "\n",
        "**Core objective**: Risk mitigation through variance reduction.\n",
        "\n",
        "**Execution model**: Multiple models are trained in parallel on randomly resampled datasets.\n",
        "\n",
        "**Error strategy**: Treats all observations as equal—no prioritization of failure cases.\n",
        "\n",
        "**Enterprise value**: Improves stability and robustness, especially for high-variance models like Decision Trees.\n",
        "\n",
        "**Typical use case**: When overfitting is the primary bottleneck.\n",
        "\n",
        "**Example**: Random Forest\n",
        "\n",
        "**Boosting**\n",
        "\n",
        "**Core objective**: Performance acceleration via bias reduction.\n",
        "\n",
        "**Execution model**: Models are trained sequentially, each iteration focusing on prior errors.\n",
        "\n",
        "**Error strategy**: Misclassified data points are up-weighted to drive continuous improvement.\n",
        "\n",
        "**Enterprise value**: Delivers higher predictive accuracy on complex patterns.\n",
        "\n",
        "**Typical use case**: When underfitting or weak learners limit business outcomes.\n",
        "\n",
        "**Example**: AdaBoost, Gradient Boosting, XGBoost\n"
      ],
      "metadata": {
        "id": "QY-mecKPVpmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is bootstrap sampling, and what role does it play in Bagging methods\n",
        "like Random Forest?**"
      ],
      "metadata": {
        "id": "sJsqdelFP9Fx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** **Strategic Role in Bagging & Random Forest**\n",
        "\n",
        "*   In Bagging (Bootstrap Aggregating), each base learner (e.g., a Decision Tree) is trained on a different bootstrap sample, creating model diversity.    \n",
        "\n",
        "* In Random Forest, bootstrap sampling de-correlates trees, reducing variance and stabilizing predictions.      \n",
        "\n",
        "*   The observations not selected in a bootstrap sample become Out-of-Bag (OOB) data, which enables internal performance validation without a separate test set.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8vrjGx4qVHf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are Out-of-Bag (OOB) samples, and how is the OOB score used to\n",
        "evaluate ensemble models?**"
      ],
      "metadata": {
        "id": "_XPMHYFuQDNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** Out-of-Bag (OOB) samples are the data points not selected during bootstrap sampling when training ensemble models like Bagging or Random Forests.  \n",
        "**Strategic Value Proposition:**\n",
        "\n",
        "*   Each base model is trained on ~63% of the data.\n",
        "*   The remaining ~37% automatically becomes OOB data. These OOB samples act as a built-in validation set.    \n",
        "**OOB Score – Performance KPI:**\n",
        "\n",
        "*   Predictions on OOB samples are aggregated across models.\n",
        "*   The resulting OOB score provides a near-unbiased estimate of model accuracy.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qIMGtOasUN1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest**"
      ],
      "metadata": {
        "id": "sf95oX6xQLMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** **Decision Trees explain decisions**   \n",
        "They provide transparent, rule-based logic that enables straightforward stakeholder communication, auditability, and rapid diagnostic insight. However, they operate with limited resilience and are susceptible to variance and overfitting.\n",
        "\n",
        "**Random Forests optimize decisions**     \n",
        "They leverage ensemble intelligence to drive higher accuracy, stability, and generalization at scale. While individual decision paths are less interpretable, the aggregate outcome is materially stronger for production-grade AI systems."
      ],
      "metadata": {
        "id": "eKMhBGTlTyqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to:   \n",
        "● Load the Breast Cancer dataset using   \n",
        "sklearn.datasets.load_breast_cancer()      \n",
        "● Train a Random Forest Classifier   \n",
        "● Print the top 5 most important features based on feature importance scores.**"
      ],
      "metadata": {
        "id": "fFD5piBOQP6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Step 2: Train a Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X, y)\n",
        "\n",
        "# Step 3: Extract feature importances\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "# Step 4: Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\"Feature\": feature_names,\n",
        "    \"Importance\": importances})\n",
        "\n",
        "# Step 5: Sort features by importance (descending order)\n",
        "top_5_features = feature_importance_df.sort_values(by=\"Importance\", ascending=False).head(5)\n",
        "\n",
        "# Step 6: Print top 5 important features\n",
        "print(top_5_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVDTckmBTVVB",
        "outputId": "cbb6e300-1f4b-4318-915d-85498e6384d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to:    \n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset     \n",
        "● Evaluate its accuracy and compare with a single Decision Tree**"
      ],
      "metadata": {
        "id": "BsJYpym9QcGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import core libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree (Baseline)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "dt_predictions = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "\n",
        "\n",
        "# Bagging Classifier (Ensembles)\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(),n_estimators=100,random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "\n",
        "bagging_predictions = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAfI5sKfS4GY",
        "outputId": "6cddc1ef-7b7a-4751-8c42-9a9f14d992f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:      \n",
        "● Train a Random Forest Classifier     \n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV     \n",
        "● Print the best parameters and final accuracy**  "
      ],
      "metadata": {
        "id": "HY9iBwAeQkPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\"n_estimators\": [50, 100, 200],\n",
        "    \"max_depth\": [None, 5, 10]}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=rf,param_grid=param_grid,cv=5,scoring=\"accuracy\")\n",
        "\n",
        "# Fit model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model from GridSearch\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Final accuracy\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Final Accuracy:\", final_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YulZrrBSoGN",
        "outputId": "43cab718-01d5-445a-dc62-9694e8177d60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:      \n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset    \n",
        "● Compare their Mean Squared Errors (MSE)**"
      ],
      "metadata": {
        "id": "QQcjZmmFQrdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "bagging_model = BaggingRegressor(random_state=42)\n",
        "random_forest_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Train the models\n",
        "bagging_model.fit(X_train, y_train)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "bagging_predictions = bagging_model.predict(X_test)\n",
        "rf_predictions = random_forest_model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "bagging_mse = mean_squared_error(y_test, bagging_predictions)\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# Print results\n",
        "print(\"Bagging Regressor MSE:\", bagging_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7_qbF3HSbae",
        "outputId": "5a6f67a5-4f4e-4c96-f3b7-5b3dd93eb0f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.2824242776841025\n",
            "Random Forest Regressor MSE: 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working as a data scientist at a financial institution to predict loan     \n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.   \n",
        "Explain your step-by-step approach to:    \n",
        "● Choose between Bagging or Boosting     \n",
        "● Handle overfitting    \n",
        "● Select base models    \n",
        "● Evaluate performance using cross-validation      \n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.**"
      ],
      "metadata": {
        "id": "4_b_KmhkQxm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Simulated loan default dataset\n",
        "X, y = make_classification(n_samples=1000,n_features=20,n_informative=10,n_redundant=5,random_state=42)\n",
        "\n",
        "# Bagging model\n",
        "bagging_model = RandomForestClassifier(n_estimators=100,random_state=42)\n",
        "\n",
        "# Boosting model\n",
        "boosting_model = GradientBoostingClassifier(n_estimators=100,learning_rate=0.1,max_depth=3,random_state=42)\n",
        "\n",
        "# Cross-validation evaluation\n",
        "bagging_score = cross_val_score(bagging_model, X, y, cv=5).mean()\n",
        "boosting_score = cross_val_score(boosting_model, X, y, cv=5).mean()\n",
        "\n",
        "print(\"Bagging (Random Forest) Accuracy:\", round(bagging_score, 4))\n",
        "print(\"Boosting (Gradient Boosting) Accuracy:\", round(boosting_score, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v3MXdcCR1Xm",
        "outputId": "3a88a2ef-32db-4ee7-af84-1cffec275130"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging (Random Forest) Accuracy: 0.933\n",
            "Boosting (Gradient Boosting) Accuracy: 0.918\n"
          ]
        }
      ]
    }
  ]
}